---
title: "Probability and Statistics II Final Project"
author: "Jake Bodea"
date: "28 April 2021"
output: 
  pdf_document:
    latex_engine: xelatex
urlcolor: blue
---

```{r, include=FALSE}
# Do not edit this code block/chunk!
knitr::opts_chunk$set(
  echo = TRUE, message=FALSE, warning = FALSE, fig.width = 16/4, 
  fig.height = 9/4
)
```

<!-- Load needed packages -->

```{r, include = FALSE}
library(dplyr)
library(ggplot2)
library(tidyverse)
library(moderndive)
```


# INTRODUCTION  

As an avid soccer fan my whole life, I've come to appreciate the joy of rooting for a team and living under the anticipation of how well they will perform in their league. European soccer leagues, which will be the basis for my data, are typically a league of about 20 teams from each country, where each team plays each other twice, once at home and once away. A win is worth 3 points, a draw 1, and a loss 0. Over the course of roughly 38 games, teams try to collect the highest score possible in an effort to stand alone atop the league table and win the trophy. 


For this project, I hope to use data collected from the 2019-2020 season of soccer across the top 5 European leagues (Spain, England, Germany, France, and Italy) to predict the league position of a European team, given it's previous season's total `xG` and `xGA`, and also the `value` of the squad before the new season begins. These statistics will be explained later in the paper.

The data set used in this research was gathered by me via the use of various sources, namely Understat.com and TransferMarkt.us. Both of these sites are unbiased, third-party sources simply presenting statistics generated by higher-level computers. Observe below a sample of a couple rows of the data frame. 

```{r, echo = FALSE}
soccer_stats <- read_csv("soccer_stats.csv")
soccer_stats %>% sample_n(size = 8)
```


## Explanation of Variables 

The first three variables `team_name`, `country`, and `leage_pos` are quite self-explanatory. Allow me now to explain the other three. 

* First, `xG` is a statistic that measures the percentage of an average player scoring a goal from a given position. For example, a penalty kick is worth 0.76 xG, meaning that there is a 76% chance of scoring a penalty. If a team had no shots all game except two penalties, their xG for the match would be the addition of those two percentages, or 1.52 xG. If that team scored both penalties, they outperformed their xG by 2 â€“ 1.52 = 0.48. xG takes into account the location of the shooter, the body part shot with, the type of pass before the goal, and the type of attack. The use of `xG` in the data frame is the season total `xG` of each team mentioned, so values of, say 65.61 xG would refer to a team being expected to score roughly that many goals in the given league season. 
* Next, `xGA` is very similar to `xG`, except that it covers more of the defensive perspective of a team. While a team with a great offense would have a high `xG`, a team with a great defense would have a low `xGA`. Thus, the benefit of the two stats has an inverse relationship. 
* Finally, the transfer market `value` of a team is the sum of all the individual values of the players in the squad. This value is determined by a player's form, age, skill, position, etc. Practically, this means that the best teams should be worth the most money, for they are made up of the best players. 


# PROCEDURES 

The goal of this project is to use multiple regression analysis to create an algorithm that would predict the `league_pos` of a team based on it's previous season's `xG` and `xGA`, as well as the current season's market `value`. In other words, considering independence between the variables, I hope to derive a formula in the structure of $\hat{y} = \widehat{league\_pos} = \beta_0 + \beta_{xG} \cdot \textrm{xG} + \beta_{xGA} \cdot \textrm{xGA} + \beta_{value} \cdot \textrm{value}$, where $\beta$ refers to the slope of a given variable. This should be done validated via inference of regression. 

Additionally, a hypothesis test would be done on each variable's slope to determine whether or not it is statistically sound to consider it a factor in `league_pos`. This would appear as $H_0: \beta = 0$ against a two-sided alternative hypothesis $H_1: \beta \ne 0$ with a value of $\alpha = 0.05$.



# RESULTS

## Multiple Regression and Inference of Regression

In order to accept a multiple linear regression model for the data, we must first analyze the variables and ensure that there is no bias or issue with the relationships between each other and the potential prediction formula. Recall that Inference of Regression requires 

1. **L**inearity of relationship between variables
2. **I**ndependence of the residuals
3. **N**ormality of the residuals
4. **E**quality of variance of the residuals

In order to test the variables, we first need to find a model (formula) by which we can test different aspects of the data. 

```{r}
pos_model <- lm(league_pos ~ xG + xGA + value, data = soccer_stats)
get_regression_table(pos_model) 
```

According to the table, our previous equation $\hat{y} = \widehat{league\_pos} = \beta_0 + \beta_{xG} \cdot \textrm{xG} + \beta_{xGA} \cdot \textrm{xGA} + \beta_{value} \cdot \textrm{value}$ can now be simplified to $$ \widehat{league\_pos} = 9.818 - 0.203 \cdot \textrm{xG} + 0.221 \cdot \textrm{xGA} - 0.001 \cdot \textrm{value}$$

We may now proceed to ensure that each of the four conditions above are met. 


### Linearity of Relationship 
Let us determine that each of the variables do in fact share a linear relationship with `league_pos`. 

#### xG and League Position


```{r, echo = FALSE}
ggplot(soccer_stats, aes(x = xG, y = league_pos)) +
  geom_point() + 
  labs(title = "Linearity of xG to League Position") + 
  geom_smooth(method = "lm", col = "blue", se = FALSE)
```


Observing the graph above, it would appear that `leage_pos` ~ `xG` have a negative linear relationship. This would mean that the higher the `xG`, the "lower" the league position. 

*Note the interesting contradiction that in terms of mathematics, being 1st in the league is equivalent to being the lowest, so negative correlations are beneficial and preferred. *

#### xGA and League Position



```{r, echo = FALSE}
ggplot(soccer_stats, aes(x = xGA, y = league_pos)) +
  geom_point() + 
  labs(title = "Linearity of xGA to League Position")+ 
  geom_smooth(method = "lm", col = "red", se = FALSE)
```

Clearly, `xGA` and `league_pos` maintain a positive linear relationship. This means that as `xGA` increases (meaning that more goals are allowed, which means that a team's defense is performing more poorly), the team's league position rises. Thus, an increasing `xGA` is something teams try to avoid. 



#### Tranfermarket Value and League Position 




```{r, echo = FALSE}
ggplot(soccer_stats, aes(x = value, y = league_pos)) +
  geom_point() + 
  labs(x = "Transfer Value (millions)", 
       title = "Linearity of Transfer Value to League Position") + 
  geom_smooth(method = "lm", col = "green4", se = FALSE)
```


Finally, there appears to be a negative linear relationship between `value` and `league_pos`, meaning that according to this data, the greater the transfer `value` of a team, the smaller the league position would be. 


#### Correlation between Variables 

Notice below the table detailing the correlations between the four variables. 

```{r}
soccer_stats %>%
  select(league_pos, xG, xGA, value) %>%
  cor()
```

As expected, `xG` and `value` maintained a moderately negative correlation with `leage_pos`, evidenced by values of about -0.6667 and -0.644 respectively. In addition, `xGA` follows a moderately positive correlation with `leage_pos` as seen by the value of 0.594 . Thus, each of the variables seem to have a moderately linear relationship with the outcome variable `leage_pos`. 


### Independence of the Residuals 
Given that the data represents each team only once, there is no repetition in any of the teams presented. This can be guaranteed because I was the one who collected the data. 

### Normality of Residuals 

```{r}
pos_model_reg_pts <- get_regression_points(pos_model)
pos_model_reg_pts %>% sample_n(size = 4)
```

The regression points table modifies the original data frame to only select the variables we are working with, then uses the formula in the model given (`lm(leage_pos ~ xG + xGA + value)`) to predict `league_pos_hat` and calculate the residual, which is the  value of `leage_pos` - `league_pos_hat`. We can now use these data points to graph a histogram of the residuals.  

```{r, echo = FALSE}
ggplot(pos_model_reg_pts, aes(x = residual)) +
  geom_histogram(binwidth = 1, col = "white") + 
  labs(title= "Approx. Bell Curve of the Residuals")
```

While not a perfect normal distribution, it would seem that the values do follow close to a bell curve, with the mean being somewhere around 0. I would say that the residuals do indeed follow a normal distribution. 


### Equality of Variance 

Since there is no "all encompassing" value by which the residual may be tested, we will analyze the equality of variance in each of the explanatory variables. 

```{r, echo=FALSE}
ggplot(pos_model_reg_pts, aes(x = xG, y = residual)) +
  geom_point() + 
  geom_hline(yintercept = 0, col = "blue", size = 2) + 
  labs(title = "Variance of Residuals of xG")
```

The graph above does not seem to change the variance of the `residual` based on the value of `xG`. 




```{r, echo=FALSE}
ggplot(pos_model_reg_pts, aes(x = xGA, y = residual)) +
  geom_point() + 
  geom_hline(yintercept = 0, col = "red", size = 2) +
  labs(title = "Variance of Residuals of xGA")
```

Similarly, `xGA` does not seem to follow any trend of a changing variance due to a change in `xGA`. 




```{r, echo=FALSE}
ggplot(pos_model_reg_pts, aes(x = value, y = residual)) +
  geom_point() + 
  geom_hline(yintercept = 0, col = "green4", size = 2) +
  labs(title = "Variance of Residuals of Value")
```

Finally, the graph once again demonstrates that there is no real change in the variance of the residuals as the `value` changes. 

Thus, the equality of variance is maintained in each of the explanatory variables. 


### Inference of Regression Conclusion:
Thus, the findings are as follows: 

1. Linearity of Variables: Yes 
2. Independence of Residuals: Yes
3. Normality of Residuals: Yes 
4. Equality of Variances: Yes 

Thus, we found that the preliminary conditions of our analysis have been met, so we can put more faith in our model and trust in the $p$-values and confidence intervals explored in the next sections. 

## Hypothesis Tests of Regression 

Now that we have determined the solidity of our model, we may begin analysis of the multiple linear regression model we had discovered above: $$ \hat{y} = \widehat{league\_pos} =9.818-0.203 \cdot \textrm{xG} + 0.221 \cdot \textrm{xGA} - 0.001 \cdot \textrm{value} $$.

Recall that we are trying to verify that our slopes are each valid estimates for the `leage_pos`. That is, $H_0: \beta = 0$ against a two-sided alternative hypothesis $H_1: \beta \ne 0$ with a value of $\alpha = 0.05$.

Observe the regression table below: 
```{r, echo=FALSE}
get_regression_table(pos_model)
```

Looking at the $p$-values for each of the columns above, we reject $H_0$ (because the $p$-values are $< 0.05 = \alpha$) for all but $\beta_{value}$. In other words, we can say with 95% confidence that each of these data values listed below have a linear relationship with `leage_pos` and that: 


* $\beta_0 \Rightarrow [6.406, 13.231]$ 
* $\beta_{xG} \Rightarrow [-0.264, -0.142]$
* $\beta_{xGA} \Rightarrow [0.169, 0.274]$


Since the $p$-value for $\beta_{value} = 0.445 > 0.05 = \alpha$, we fail to reject $H_0$, so we cannot say that it is a factor in the multiple linear regression formula. Therefore, our hypothesis testing found that a statistically sound prediction for `league_pos` is:

$$ \widehat{league\_pos} = 9.818 - 0.203 \cdot \textrm{xG} + 0.221 \cdot \textrm{xGA}$$


# CONCLUSION

Using multiple regression, inference for regression, and hypothesis testing for regression, we were able to determine the following formula: 

$$ \widehat{league\_pos} = 9.818 - 0.203 \cdot \textrm{xG} + 0.221 \cdot \textrm{xGA}$$

Inference for regression allowed us to verify that the results of our hypothesis tests and confidence intervals do indeed have a valid meaning. Hypothesis testing of $H_0: \beta = 0$ against the two-sided alternate hypothesis revealed that we can say with 95% confidence that $\beta \ne 0$ for all but $\beta_{value}$, which means that the slopes between each of the remaining explanatory variables and the outcome variable are indeed statistically different from 0. 

Finally, confidence intervals demonstrated that with 95% confidence, the true value of $\beta$ is:

* $\beta_0 \Rightarrow [6.406, 13.231]$ 
* $\beta_{xG} \Rightarrow [-0.264, -0.142]$
* $\beta_{xGA} \Rightarrow [0.169, 0.274]$

## Final Remarks 

It is important to note that there are MANY other factors that could go into such a predictive algorithm, such as injuries, contract tensions, a global pandemic putting a stop to the entire world, etc. However, these findings seem to be quite accurate given the data with all other things held constant. Despite this, it is important to recall that correlation does not imply causation, so even though these explanatory variables logically make sense to have an affect of a team's league position, there is always the possibility that there is no causation between the variables. 

I'll close with this: regardless of how complex our algorithms and supercomputers are, sport is one of those things that consistently goes beyond the statistics. If this were not the case, an advanced super computer would be able to predict the brackets of March Madness, for example, yet nobody in history has come up with a correct bracket. There seems to be a beauty in the space that won't allow itself to be governed by the laws of the statistics. 

